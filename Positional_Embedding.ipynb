{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38b480aa-19c4-422f-a131-2fe81f444026",
   "metadata": {},
   "source": [
    "# POSITIONAL EMBEDDINGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c467612f-c460-4a67-b03f-533f9ca58a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS IS SMALLER THAN what the original GPT3 model used "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ede396b-dc85-4aa0-b417-5bc003f5a8cb",
   "metadata": {},
   "source": [
    "## What are Positional Embeddings?\n",
    "\n",
    "Positional embeddings are like giving each word or item in a list a **unique sticker** that tells where it is‚Äîfirst, second, third, and so on.  \n",
    "This helps a computer understand **order**, much like how humans use word order to make sense of sentences.\n",
    "\n",
    "---\n",
    "\n",
    "## Everyday Example\n",
    "\n",
    "üìù Imagine a grocery list:\n",
    "\n",
    "- \"Milk, Eggs, Bread\"\n",
    "\n",
    "If the order changes to:\n",
    "\n",
    "- \"Bread, Milk, Eggs\"\n",
    "\n",
    "‚Ä¶the meaning changes.\n",
    "\n",
    "For a computer, every word by itself‚Äî*\"Milk,\" \"Eggs,\" \"Bread\"*‚Äîlooks the same unless **extra information about its position** is included.  \n",
    "\n",
    "üëâ Positional embeddings act like writing numbers next to each item:\n",
    "\n",
    "- **1. Milk, 2. Eggs, 3. Bread**\n",
    "\n",
    "Now the computer knows **\"Milk\" comes first, \"Eggs\" second, and so on.**\n",
    "\n",
    "---\n",
    "\n",
    "## Analogy\n",
    "\n",
    "üöÇ Think of a **train with three cars**:\n",
    "\n",
    "- Engine ‚Üí at the front  \n",
    "- Passenger car ‚Üí in the middle  \n",
    "- Caboose ‚Üí at the end  \n",
    "\n",
    "If the train cars are mixed up, the train doesn‚Äôt work the same!  \n",
    "\n",
    "Computers use positional embeddings to **keep track of which \"car\" (word or item) is in which spot**, so order-sensitive tasks (translation, understanding, organizing text) work correctly.\n",
    "\n",
    "---\n",
    "\n",
    "## Why It‚Äôs Important\n",
    "\n",
    "- Without positional embeddings, a computer would see all words/items in a **jumbled, random order**.  \n",
    "- Adding positional embeddings helps models process **stories, instructions, or any ordered information** just like people do.  \n",
    "\n",
    "üí° In short:  \n",
    "Positional embedding = **labels or numbers** on each item saying:  \n",
    "*\"This comes first, this comes next, and so on.\"*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7c52b3f-c350-45ad-8a58-3052e9f90df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=50257\n",
    "output_dim=256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9463f54b-6d76-478f-99e4-7b1f20830254",
   "metadata": {},
   "source": [
    "vocab_size=50257 and output_dim=256 are settings that define how a language model \"sees\" and processes text, and they are usually declared at the very start of the model's code to set up its basic structure.\n",
    "\n",
    "## What Each Parameter Means\n",
    "vocab_size=50257: This is the number of unique words or symbols (called \"tokens\") the model understands. For example, there can be codes for common words, punctuation, and even emoji. 50257 means the model can recognize 50,257 different tokens.\n",
    "\n",
    "output_dim=256: This sets the size of each token's \"embedding,\" which is like the size of a fingerprint representing each word. Each word gets a unique numeric fingerprint that is 256 numbers long, so the model can understand and compare meanings.\n",
    "\n",
    "\n",
    "## Why Declare These First\n",
    "These two values define the \"input language\" and the \"representation size\" for words the model will use everywhere else. It is like deciding up front how many words will be in a dictionary and how big the pages will be for writing explanations about each word.\n",
    "\n",
    "Declaring them first keeps the model structure clear and consistent, and ensures every word fits in the model's \"mental space\".\n",
    "\n",
    "## Simple Analogy\n",
    "Imagine building a library:\n",
    "\n",
    "vocab_size: Decides how many books are on the shelves.\n",
    "\n",
    "output_dim: Decides how many pages are in each book to store information about each topic.\n",
    "\n",
    "These numbers are written first so the library builders know the space and organization needed for everything else to work smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28f410a5-37db-4a27-8dd2-2d20010caafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a401578-947d-412d-b71f-eba3432cd905",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding_layers=torch.nn.Embedding(vocab_size,output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938a7a37-5a80-4c59-843a-207f83ed87e4",
   "metadata": {},
   "source": [
    "## What does this do?\n",
    "\n",
    "`torch.nn.Embedding` is like a **lookup table** that holds a big list (matrix) of vectors (lists of numbers).\n",
    "\n",
    "- Each word/token in your vocabulary has a unique number (**index**).\n",
    "- This layer maps that number (index) to a vector of size `output_dim` (length **256** in your case).\n",
    "\n",
    "üëâ Example:  \n",
    "If `vocab_size = 50257` and `output_dim = 256`, the embedding table has:\n",
    "\n",
    "- **50257 rows** ‚Üí one for each token  \n",
    "- **256 columns** ‚Üí embedding size  \n",
    "\n",
    "When you input a token‚Äôs index, this layer gives you the **corresponding vector (representation).**\n",
    "\n",
    "---\n",
    "\n",
    "## Why use this?\n",
    "\n",
    "- Converts tokens (just numbers) into **meaningful dense vectors** that capture semantic information.  \n",
    "- These vectors are **learned during training**, so the model can understand **connections between words**.  \n",
    "- Unlike **one-hot encoding** (big, sparse vectors), embeddings provide a **compact and useful representation**.\n",
    "\n",
    "---\n",
    "\n",
    "## Simple Analogy\n",
    "\n",
    "üìñ Imagine a **dictionary (table)** with **50257 words (rows)**.  \n",
    "\n",
    "- Each word is associated with a **256-dimensional \"fingerprint\" (vector)** representing its meaning.  \n",
    "- When you give the embedding layer a **word's ID**, it simply **looks up and returns that word's fingerprint**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b5d995-75e0-47e6-aeed-7d81fa8cf7d5",
   "metadata": {},
   "source": [
    "## What does DataLoader do?\n",
    "\n",
    "When we have a **very large dataset**, it‚Äôs not practical to give the entire dataset to the model at once.  \n",
    "üëâ `DataLoader` solves this problem by splitting the data into **small batches**.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Batching**  \n",
    "  Breaks large data into **mini-batches**.  \n",
    "  Example: Like dividing a big group into smaller teams for easier handling.  \n",
    "\n",
    "- **Step-by-Step Feeding**  \n",
    "  These batches are provided to the model **gradually during training**, making learning efficient.  \n",
    "\n",
    "- **Shuffling**  \n",
    "  Can shuffle the data (randomize the order), so the model sees data in different sequences.  \n",
    "  üîÑ This prevents the model from memorizing patterns and helps it learn better.  \n",
    "\n",
    "- **Parallel Loading**  \n",
    "  Uses **multiple threads/workers** in the background to load data faster.  \n",
    "  ‚ö° Ensures the model never has to \"wait\" for data.  \n",
    "\n",
    "---\n",
    "\n",
    "## Simple Analogy\n",
    "\n",
    "üì¶ Imagine you need to move **1000 books** to another room.  \n",
    "Instead of carrying them all at once (impossible), you divide them into **small boxes (batches)** and carry them step by step.  \n",
    "\n",
    "Similarly, `DataLoader` prepares data in **manageable pieces** so training is smooth and efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a2948b7-35ad-478a-b2e7-e68772ac79d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2deca083cf75493f91e315a0ac8d529d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kushagra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\kushagra\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4df62256a7647a0a1ab32be33152588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c73d5c000c47c597d43680a05e4e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5713b875209d4ebb986ba4baf8453cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fea1d1de5f14ee7a267b4ba3aabcb0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5145 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 128])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Step 1: Read raw text from file\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# Step 2: Initialize GPT2 tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Step 3: Tokenize raw text into token ids\n",
    "tokenized_text = tokenizer.encode(raw_text)\n",
    "\n",
    "# Step 4: Define custom dataset class\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, tokenized_text, max_length, stride):\n",
    "        self.tokenized_text = tokenized_text\n",
    "        self.max_length = max_length\n",
    "        self.stride = stride\n",
    "        self.samples = self.create_samples()\n",
    "\n",
    "    def create_samples(self):\n",
    "        samples = []\n",
    "        for i in range(0, len(self.tokenized_text) - self.max_length + 1, self.stride):\n",
    "            sample = self.tokenized_text[i:i+self.max_length]\n",
    "            samples.append(sample)\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.samples[idx], dtype=torch.long)\n",
    "\n",
    "# Step 5: Create data loader function\n",
    "def create_dataloader_v1(tokenized_text, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    dataset = GPTDatasetV1(tokenized_text, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "# Step 6: Use the dataloader on your tokenized data\n",
    "dataloader = create_dataloader_v1(tokenized_text, batch_size=8, max_length=128, stride=64, shuffle=True)\n",
    "\n",
    "# Step 7: Iterate batches\n",
    "data_iter = iter(dataloader)\n",
    "batch = next(data_iter)\n",
    "print(batch.shape)  # Example output: torch.Size([8, 128])\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca42eee7-4f33-4ce4-8d63-7b8a1d2afccf",
   "metadata": {},
   "source": [
    "max_length=4\n",
    "dataloader=create_dataloader_v1(\n",
    "    raw_text,batch_size=8,max_length=max_length,stride=max_length,shuffle=False\n",
    ")\n",
    "data_iter=iter(dataloader)\n",
    "inputs,target=next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eacb900e-b27d-4573-ba49-96ca9ad67e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4        # Maximum allowed for GPT2\n",
    "stride = 4              # For overlapping chunks, or use 1024 for non-overlapping\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    tokenized_text,       # Your list of token ids\n",
    "    batch_size=8,\n",
    "    max_length=max_length,\n",
    "    stride=stride,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7a234fb-bd0d-423e-b92d-f41c7692902a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID'S:\n",
      " tensor([[  287,   262,  6001,   286],\n",
      "        [  465, 13476,    11,   339],\n",
      "        [  550,  5710,   465, 12036],\n",
      "        [   11,  6405,   257,  5527],\n",
      "        [27075,    11,   290,  4920],\n",
      "        [ 2241,   287,   257,  4489],\n",
      "        [   64,   319,   262, 34686],\n",
      "        [41976,    13,   357, 10915]])\n",
      "\n",
      "Input shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "targets = iter(dataloader)\n",
    "inputs = next(data_iter)  # 'inputs' variable ab define ho gaya\n",
    "print(\"Token ID'S:\\n\", inputs)\n",
    "print(\"\\nInput shape:\\n\", inputs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d382bad-4097-479f-9b0b-e28af794c9ee",
   "metadata": {},
   "source": [
    "## Step-by-Step Explanation\n",
    "\n",
    "### 1. Create an Iterator\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "We create an iterator from the dataloader.\n",
    "\n",
    "This allows us to fetch data batch by batch.\n",
    "\n",
    "An iterator lets us access data sequentially, one piece at a time.\n",
    "\n",
    "### 2. Get the Next Batch\n",
    "\n",
    "inputs = next(data_iter)\n",
    "We fetch the next batch from the iterator (here, the first batch).\n",
    "\n",
    "Each batch is usually a tensor containing token IDs.\n",
    "\n",
    "Typical shape: (batch_size, sequence_length)\n",
    "\n",
    "### 3. Inspect the Tokens\n",
    "print(\"Token ID's:\\n\", inputs)\n",
    "Prints the actual token IDs inside the batch.\n",
    "\n",
    "This shows which tokens are present in the current batch.\n",
    "\n",
    "### 4. Check Batch Shape\n",
    "print(\"\\nInput shape:\\n\", inputs.shape)\n",
    "Prints the shape of the batch.\n",
    "\n",
    "Example: torch.Size([8, 4])\n",
    "\n",
    "\n",
    "8 ‚Üí number of samples in the batch (batch_size)\n",
    "\n",
    "4 ‚Üí number of tokens per sample (sequence_length)\n",
    "\n",
    "## Simple Analogy\n",
    "üì¶ Think of dataloader as a conveyor belt of small boxes (batches).\n",
    "\n",
    "iter(dataloader) gives you access to the belt.\n",
    "\n",
    "next(data_iter) lets you pick the next box.\n",
    "\n",
    "Each box has items (token IDs) neatly arranged in rows (batch_size) and columns (sequence_length).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7f2e20ce-06e4-4fed-b506-fd8e8f330910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings=token_embedding_layers(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1163af17-ae53-4f07-8ca1-16abc924a904",
   "metadata": {},
   "source": [
    "## Token Embedding Layers in PyTorch\n",
    "\n",
    "`token_embedding_layers` ek **PyTorch embedding layer** hai jo har **token ID** ko ek numeric vector (embedding) mein convert karti hai.  \n",
    "\n",
    "- **Inputs** ‚Üí Token IDs (for example: ek batch ke 8 sequences, har sequence mein 4 token IDs)  \n",
    "- **Forward pass** ‚Üí `token_embedding_layers(inputs)` likhne par, yeh layer har token ID ke liye ek **256-dimensional vector** (embedding) return karti hai.  \n",
    "\n",
    "### Output Shape\n",
    "```python\n",
    "print(token_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3809b30-e954-479b-b1a9-31cac0c8f953",
   "metadata": {},
   "source": [
    "Shape hota hai: (batch_size, sequence_length, embedding_dimension)\n",
    "\n",
    "Example: (8, 4, 256)\n",
    "\n",
    "8 ‚Üí batch size (8 sequences)\n",
    "\n",
    "4 ‚Üí sequence length (har sequence mein 4 tokens)\n",
    "\n",
    "256 ‚Üí embedding dimension (har token ka 256-dim vector)\n",
    "\n",
    "üìñ Intuition:\n",
    "Har word ek unique 256-number fingerprint se represent hota hai.\n",
    "Toh 8 sequences √ó 4 tokens √ó 256-dim vectors = ek 3D tensor jo model ko dena easy aur meaningful hota hai."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ca2b1-eacc-4a5c-b62c-b0a9f9d274ce",
   "metadata": {},
   "source": [
    "* As we can tell that 8x4x256 -dimension tensor output,each token ID is now embedded as a 256 dimensional vector *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aef2b301-fe7b-4459-987a-d3ec3e2f7d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length=max_length\n",
    "pos_embedding_layer=torch.nn.Embedding(context_length,output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2badfd-3b6d-4ba9-b4f5-c39c83170c64",
   "metadata": {},
   "source": [
    "## Explanation of DataLoader batch iteration and token embedding process\n",
    "\n",
    "1. **DataLoader se batch lena:**\n",
    "\n",
    "data_iter = iter(dataloader) # DataLoader object se iterator banaate hain\n",
    "inputs = next(data_iter) # Iterator se ek batch data nikalte hain\n",
    "\n",
    "text\n",
    "\n",
    "- `dataloader` se hum batches me data lete hain, taki training efficient ho.\n",
    "- `iter()` se iterator banta hai jo ek time me ek batch deta hai.\n",
    "- `next()` se agla batch milta hai.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Batch ke token IDs aur unka shape print karna:**\n",
    "\n",
    "print(\"Token ID'S:\\n\", inputs) # Batch ke token IDs dikhate hain\n",
    "print(\"\\nInput shape:\\n\", inputs.shape) # Batch ka size ya shape batate hain (batch_size, sequence_length)\n",
    "\n",
    "text\n",
    "\n",
    "---\n",
    "\n",
    "3. **Token embedding layer apply karna:**\n",
    "\n",
    "token_embeddings = token_embedding_layers(inputs)\n",
    "print(token_embeddings.shape)\n",
    "\n",
    "text\n",
    "\n",
    "- Token IDs ko embedding vectors me badal dete hain.\n",
    "- Embedding se har token ko ek numeric vector milta hai, jiska dimension `output_dim` hota hai.\n",
    "- Output tensor ka shape hota hai `(batch_size, sequence_length, output_dim)`.\n",
    "- Jaise ki `(8, 4, 256)` matlab 8 sequences, 4 tokens har sequence me, aur har token 256-dimension vector ke roop me.\n",
    "\n",
    "---\n",
    "\n",
    "4. **Position embedding define karna:**\n",
    "\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "text\n",
    "\n",
    "- Har **position** (index 0,1,2,...) ko bhi embedding vector milta hai.\n",
    "- Position embeddings token embeddings me add karke model ko word order samjhate hain.\n",
    "\n",
    "---\n",
    "\n",
    "### Kyun zaruri hai positional embedding?\n",
    "\n",
    "- Transformers parallel me words process karte hain, isliye word order samajhna zaruri hota hai.\n",
    "- Position embedding model ko bataata hai ki har token sequence me kis position pe hai.\n",
    "- Isse sentence ka structure samajh me aata hai aur meaning sahi hota hai.\n",
    "\n",
    "---\n",
    "\n",
    "Ye step-by-step batata hai ki kaise DataLoader ka use karke tokens ko batch me lekar embeddings banate hain, aur unme position information add karte hain jo models ke accuracy badhate hain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43ac7ba7-76d8-45b4-9b2a-9bc6809915d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings=pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b47894-fd21-4eaa-936b-a28869aa4d55",
   "metadata": {},
   "source": [
    "# PyTorch Basics: Embeddings, Positional Embeddings, DataLoader & Iterators\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Token Embedding Layer\n",
    "\n",
    "`torch.nn.Embedding` is like a **lookup table** that maps each token ID to a vector (embedding).\n",
    "\n",
    "- Input: Token IDs (numbers)  \n",
    "- Output: Dense vectors (embeddings)\n",
    "\n",
    "üëâ Example:  \n",
    "If `vocab_size = 50257` and `output_dim = 256` ‚Üí the table has **50257 rows √ó 256 columns**.  \n",
    "\n",
    "### Example Shape\n",
    "- Input batch: `(8, 4)` ‚Üí 8 sequences, 4 tokens each  \n",
    "- Output embeddings: `(8, 4, 256)` ‚Üí every token becomes a 256-dimensional vector  \n",
    "\n",
    "üìñ Analogy: Like a dictionary where each word has a **256-number fingerprint**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Positional Embeddings\n",
    "\n",
    "Embeddings tell the **meaning** of tokens, but not their **order**.  \n",
    "Positional embeddings add order information.\n",
    "\n",
    "- They act like **stickers or labels**: first, second, third‚Ä¶  \n",
    "- Without them, a computer sees tokens as unordered.\n",
    "\n",
    "### Everyday Example\n",
    "- Grocery list: *‚ÄúMilk, Eggs, Bread‚Äù*  \n",
    "- Change order to *‚ÄúBread, Milk, Eggs‚Äù* ‚Üí meaning changes!  \n",
    "- Positional embeddings are like numbering: **1. Milk, 2. Eggs, 3. Bread**  \n",
    "\n",
    "üöÇ Analogy: Train cars (engine, passenger, caboose) ‚Üí if shuffled, the train won‚Äôt work.  \n",
    "Computers need positional embeddings to keep tokens in the **right order**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. DataLoader\n",
    "\n",
    "When data is very large, we can‚Äôt feed it all at once.  \n",
    "üëâ `DataLoader` breaks it into **mini-batches**.\n",
    "\n",
    "### Key Features\n",
    "- **Batching** ‚Üí splits data into smaller parts  \n",
    "- **Step-by-step feeding** ‚Üí model trains batch by batch  \n",
    "- **Shuffling** ‚Üí randomizes order for better learning  \n",
    "- **Parallel loading** ‚Üí multiple workers load data quickly  \n",
    "\n",
    "üì¶ Analogy: Moving 1000 books ‚Üí instead of all at once, carry in **small boxes (batches)**.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Iterators with DataLoader\n",
    "\n",
    "We use an **iterator** to fetch data batch by batch:\n",
    "\n",
    "```python\n",
    "data_iter = iter(dataloader)       # create iterator\n",
    "inputs = next(data_iter)           # get first batch\n",
    "\n",
    "print(\"Token IDs:\\n\", inputs)      # show token IDs\n",
    "print(\"Input shape:\\n\", inputs.shape)  # e.g., torch.Size([8, 4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373e3da4-bc46-4bb5-8b1c-ed9153c46b92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00098239864c4851847ecd5dd4cde362": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "11dd52a168cd4d3c8afae0831a20cce6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1afcdd920b114aafb7430976e085e285": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1fea1d1de5f14ee7a267b4ba3aabcb0c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ddea18a1f7d947cfa3aa5692684a42cb",
        "IPY_MODEL_cb940094099c488b9250590386a46d79",
        "IPY_MODEL_2a572536115f4ad4a377f86968c90b17"
       ],
       "layout": "IPY_MODEL_d6e6b7dac30b436884ab12d20439b381"
      }
     },
     "2932d7d1ae5648eca297bc3b6f2e4061": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2a50bc7e5b97452e87b9cc0ca138b62e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2a572536115f4ad4a377f86968c90b17": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b327f2b506354f7f90df0fa87d57442c",
       "style": "IPY_MODEL_96a13c4aa6304b6fb6684114b40d7686",
       "value": "‚Äá665/665‚Äá[00:00&lt;00:00,‚Äá32.8kB/s]"
      }
     },
     "2deca083cf75493f91e315a0ac8d529d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_8af442d07b934537b7de88eac13a2533",
        "IPY_MODEL_6137914d9d854fc9a1b8b602f88b3222",
        "IPY_MODEL_a86ea5eaf247455dbc1864d4df7f8c8b"
       ],
       "layout": "IPY_MODEL_4d6f563b0a0443a98532b353fe5d8944"
      }
     },
     "398a7fc57c274ff0b20c65dcf36c0470": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_43366d735ceb415395d2858a8c57fc87",
       "max": 1355256,
       "style": "IPY_MODEL_2a50bc7e5b97452e87b9cc0ca138b62e",
       "value": 1355256
      }
     },
     "3bd772273f1e43c8af7ef1fc4be9adbb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_90a97a9d13714ad28c341b61bae8a726",
       "max": 456318,
       "style": "IPY_MODEL_d78a71c57d6d4993bbe38c765939f89e",
       "value": 456318
      }
     },
     "4150c7587524436aac638e061120ef5a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_84729a526fa14dd18df3a8fdc4dc810c",
       "style": "IPY_MODEL_6030165edf524fe88f8f8a52a54eace6",
       "value": "merges.txt:‚Äá100%"
      }
     },
     "42fd2060143e4105b03fccdd42e7d73f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9925bdafee0c401d8efff7d1cd034fde",
       "style": "IPY_MODEL_8f3d0294155d4b58bcf3f976a602a5f2",
       "value": "‚Äá1.36M/1.36M‚Äá[00:00&lt;00:00,‚Äá2.40MB/s]"
      }
     },
     "43366d735ceb415395d2858a8c57fc87": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "45c73d5c000c47c597d43680a05e4e73": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_4150c7587524436aac638e061120ef5a",
        "IPY_MODEL_3bd772273f1e43c8af7ef1fc4be9adbb",
        "IPY_MODEL_cd32314af15a491181cf485f4424966a"
       ],
       "layout": "IPY_MODEL_b87f4ed0158847368debad1461b76e23"
      }
     },
     "4ca3cca2ea8c407b8db07016cafc6b97": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4d6f563b0a0443a98532b353fe5d8944": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "507f12b44f6544f1a397d07565f158a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "527e36e738c7432699479e29f98afa54": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "532babd745e44d6a8d7bb9959239c759": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5713b875209d4ebb986ba4baf8453cb2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_b3e1644e5d634d26927d1196ca2569d0",
        "IPY_MODEL_398a7fc57c274ff0b20c65dcf36c0470",
        "IPY_MODEL_42fd2060143e4105b03fccdd42e7d73f"
       ],
       "layout": "IPY_MODEL_00098239864c4851847ecd5dd4cde362"
      }
     },
     "59259b2027b34ebc80581bef658d713b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "59996c9b6f2d46309a7c526ecb3b153c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_11dd52a168cd4d3c8afae0831a20cce6",
       "style": "IPY_MODEL_2932d7d1ae5648eca297bc3b6f2e4061",
       "value": "‚Äá1.04M/1.04M‚Äá[00:00&lt;00:00,‚Äá1.52MB/s]"
      }
     },
     "6030165edf524fe88f8f8a52a54eace6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6137914d9d854fc9a1b8b602f88b3222": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_6d0d7d28600543d6adacadb545093b6a",
       "max": 26,
       "style": "IPY_MODEL_e5e01e250b3c45b7b5809ea934ca2d16",
       "value": 26
      }
     },
     "65da83919f9d44078d0398c43287d893": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6d0d7d28600543d6adacadb545093b6a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "72026081a5fa402ca3b1a487be29e724": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_94090924091a492ead6fe61b8ff8f670",
       "style": "IPY_MODEL_ff81a0e91d724749a818a5bd304e388c",
       "value": "vocab.json:‚Äá100%"
      }
     },
     "7471901a1ce54d3ea78a75ed68b0e702": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "7ca2160307b1468a8f6c2a76bf86dcd3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7d0275a24fdd49ffa3d8a79b9f00a307": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_4ca3cca2ea8c407b8db07016cafc6b97",
       "max": 1042301,
       "style": "IPY_MODEL_7471901a1ce54d3ea78a75ed68b0e702",
       "value": 1042301
      }
     },
     "84729a526fa14dd18df3a8fdc4dc810c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8af442d07b934537b7de88eac13a2533": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_65da83919f9d44078d0398c43287d893",
       "style": "IPY_MODEL_bdcc0bc1cb5d4e669f87c46707a442b0",
       "value": "tokenizer_config.json:‚Äá100%"
      }
     },
     "8f3d0294155d4b58bcf3f976a602a5f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "90a97a9d13714ad28c341b61bae8a726": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "94090924091a492ead6fe61b8ff8f670": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "96a13c4aa6304b6fb6684114b40d7686": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9925bdafee0c401d8efff7d1cd034fde": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a2253d8cd24a446db98a3ce86f7681ce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a4df62256a7647a0a1ab32be33152588": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_72026081a5fa402ca3b1a487be29e724",
        "IPY_MODEL_7d0275a24fdd49ffa3d8a79b9f00a307",
        "IPY_MODEL_59996c9b6f2d46309a7c526ecb3b153c"
       ],
       "layout": "IPY_MODEL_a2253d8cd24a446db98a3ce86f7681ce"
      }
     },
     "a86ea5eaf247455dbc1864d4df7f8c8b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e969e34dd7fc4b0e98d73eeec27c67f1",
       "style": "IPY_MODEL_507f12b44f6544f1a397d07565f158a0",
       "value": "‚Äá26.0/26.0‚Äá[00:00&lt;00:00,‚Äá1.26kB/s]"
      }
     },
     "ace5f76c70864694a080ebc0d646d6f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b327f2b506354f7f90df0fa87d57442c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b3e1644e5d634d26927d1196ca2569d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_532babd745e44d6a8d7bb9959239c759",
       "style": "IPY_MODEL_7ca2160307b1468a8f6c2a76bf86dcd3",
       "value": "tokenizer.json:‚Äá100%"
      }
     },
     "b87f4ed0158847368debad1461b76e23": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bdcc0bc1cb5d4e669f87c46707a442b0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c4352548500b47f788d84cfc8332dc81": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c50f1efb6306463f8f451cb5a330ddf1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cb940094099c488b9250590386a46d79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_527e36e738c7432699479e29f98afa54",
       "max": 665,
       "style": "IPY_MODEL_ace5f76c70864694a080ebc0d646d6f2",
       "value": 665
      }
     },
     "cd32314af15a491181cf485f4424966a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c50f1efb6306463f8f451cb5a330ddf1",
       "style": "IPY_MODEL_1afcdd920b114aafb7430976e085e285",
       "value": "‚Äá456k/456k‚Äá[00:00&lt;00:00,‚Äá1.84MB/s]"
      }
     },
     "d6e6b7dac30b436884ab12d20439b381": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d78a71c57d6d4993bbe38c765939f89e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "ddea18a1f7d947cfa3aa5692684a42cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c4352548500b47f788d84cfc8332dc81",
       "style": "IPY_MODEL_59259b2027b34ebc80581bef658d713b",
       "value": "config.json:‚Äá100%"
      }
     },
     "e5e01e250b3c45b7b5809ea934ca2d16": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "e969e34dd7fc4b0e98d73eeec27c67f1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ff81a0e91d724749a818a5bd304e388c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
