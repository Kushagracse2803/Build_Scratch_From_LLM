{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98584378-e00f-4a34-b582-460d0a359ce6",
   "metadata": {},
   "source": [
    "# Reading in a short story as text sample into Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380c9e8f-262b-4497-a443-a42a91ea5566",
   "metadata": {},
   "source": [
    "# Step 1: Creating Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "188cc3e1-7c73-4d7c-b366-cc8c10c62a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    raw_text=f.read()\n",
    "print(\"Total number of character\",len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aca4ef-2c55-42eb-b4af-02dae9c4b9e7",
   "metadata": {},
   "source": [
    "This Python code reads a text file named the-verdict.txt and gives you:\n",
    "\n",
    "The total number of characters in the file.\n",
    "\n",
    "The first 99 characters from the file's content.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40e7c5b0-0429-45d6-8c29-a5b2d71b4060",
   "metadata": {},
   "source": [
    "‚úÖ re Operator in Python\n",
    "The re module in Python stands for Regular Expression ‚Äî it provides tools to work with patterns in text, such as searching, matching, or extracting data.\n",
    "\n",
    "üîß Commonly Used re Functions:\n",
    "Function\tDescription\n",
    "re.search()\tSearches a pattern in a string (returns first match)\n",
    "re.match()\tChecks if the pattern matches at the beginning\n",
    "re.findall()\tReturns all matches of the pattern as a list\n",
    "re.sub()\tSubstitutes matched pattern with a replacement\n",
    "re.split()\tSplits string using a regex pattern\n",
    "re.compile()\tCompiles a pattern for reuse\n",
    "\n",
    "üß† Basic Regex Syntax:\n",
    "Pattern\tMeaning\tExample Match\n",
    ".\tAny character except newline\ta.c ‚Üí matches abc\n",
    "\\d\tDigit (0-9)\t\\d\\d ‚Üí 23\n",
    "\\w\tWord character (a-z, A-Z, 0-9, _)\t\\w+ ‚Üí Hello123\n",
    "\\s\tWhitespace\t\\s ‚Üí space/tab\n",
    "^\tStart of string\t^The matches ‚ÄúThe‚Äù at beginning\n",
    "$\tEnd of string\tend$ matches line ending in ‚Äúend‚Äù\n",
    "[abc]\tMatches a, b, or c\t[aeiou] ‚Üí vowels\n",
    "[^abc]\tNot a, b, or c\t\n",
    "*\t0 or more times\ta* ‚Üí aaa or \"\"\n",
    "+\t1 or more times\ta+ ‚Üí a, aa\n",
    "?\t0 or 1 time\ta? ‚Üí optional a\n",
    "{m,n}\tBetween m and n repetitions\ta{2,4} ‚Üí aa, aaa, aaaa\n",
    "`\t`\tOR operator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d20a4799-cd57-4ef6-bb83-21892620d750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,is', ' ', 'a', ' ', 'test']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "text=\"Hello, world. This,is a test\"\n",
    "result= re.split(r'(\\s)',text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d64ec63-3dcf-4533-aad5-d9c3ee59e6a0",
   "metadata": {},
   "source": [
    "üîç What's Happening?\n",
    "\n",
    "re.split(): Splits a string wherever the pattern matches.\n",
    "\n",
    "\n",
    "r'(\\s)':\n",
    "\n",
    "\n",
    "\\s ‚Üí Matches any whitespace (space, tab, newline).\n",
    "\n",
    "\n",
    "The pattern is inside parentheses (), which creates a capturing group.\n",
    "\n",
    "\n",
    "üìå When you use a capturing group, the delimiter (matched part) is also included in the result list.\n",
    "\n",
    "\n",
    "üîß So re.split(r'(\\s)', text) will:\n",
    "\n",
    "Split text at each space.\n",
    "\n",
    "\n",
    "Include the space itself in the result list.\n",
    "\n",
    "üîé Output:\n",
    "\n",
    "['Hello,', ' ', 'world.', ' ', 'This,is', ' ', 'a', ' ', 'test']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8486c828-aa99-458b-a6d5-574a0976dd01",
   "metadata": {},
   "source": [
    "#### SEPAARATE COMMA AS WELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9584ce74-2325-40d5-b480-14778913533c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', 'is', ' ', 'a', ' ', 'test']\n"
     ]
    }
   ],
   "source": [
    "result=re.split(r'([,.]|\\s)',text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b63dfa6-2b11-40ec-a647-a0ca7567d9f2",
   "metadata": {},
   "source": [
    "#### our list include still include whilespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20f9bfbb-56b1-4f5d-b9cd-63588a7ca2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test']\n"
     ]
    }
   ],
   "source": [
    "result=[item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3456f5b8-846e-4cf0-9bd1-7145899e4f87",
   "metadata": {},
   "source": [
    "## Removing white space or not\n",
    "‚úÖ Should You Remove Whitespace in Text Processing?\n",
    "Whether or not to remove whitespace depends on your task in NLP or text processing. Here's a balanced look at the advantages and disadvantages:\n",
    "\n",
    "‚úÖ Advantages of Removing Whitespace\n",
    "Data Cleaning:\n",
    "\n",
    "Removes unwanted leading/trailing spaces, tabs, and extra newlines.\n",
    "\n",
    "Prevents errors during tokenization and matching.\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "text = \"  Hello world  \\n\"\n",
    "clean_text = text.strip()\n",
    "Consistent Processing:\n",
    "\n",
    "Reduces noise in data.\n",
    "\n",
    "Especially useful in keyword matching or when comparing strings.\n",
    "\n",
    "Improves Storage & Speed (slightly):\n",
    "\n",
    "In very large corpora, removing extra whitespace can reduce file size.\n",
    "\n",
    "‚ùå Disadvantages of Removing Whitespace\n",
    "Loss of Structure:\n",
    "\n",
    "Whitespace separates words and sentences.\n",
    "\n",
    "Removing all whitespace can make the text unusable for NLP.\n",
    "\n",
    "\"Hello world\" ‚Üí \"Helloworld\" ‚ùå\n",
    "Hurts Tokenization:\n",
    "\n",
    "Most NLP tools rely on whitespace to split text into tokens.\n",
    "\n",
    "Removing it completely can break sentence or word boundaries.\n",
    "\n",
    "Inaccurate Analysis:\n",
    "\n",
    "In tasks like text generation, translation, or summarization, spacing is important to retain grammatical structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0338ed45-1997-415d-bcc0-1ce9dac976ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. Is this-- a test?\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60940358-df7e-4202-a82e-4918f2270115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!()\\']|--|\\s)', raw_text)\n",
    "preprocessed=[item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14594c9f-d2b9-4abf-989c-7115ddd69e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4606\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92de890-61f5-4e58-990a-1655b58bf4c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cc854f5-d27b-4b2e-8bab-50787009a753",
   "metadata": {},
   "source": [
    "# Step 2 Convert token into convert ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c92aa8f7-f460-4b50-af8a-72d235d7a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we tokenised the short strory and assigned to a python preprocessed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7af5eb7a-7904-4525-9503-69f2ade31d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1158\n"
     ]
    }
   ],
   "source": [
    "all_words=sorted(set(preprocessed))\n",
    "vocab_size=len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa96a26c-f822-48e7-aeb0-b7b120643333",
   "metadata": {},
   "source": [
    "#Now we are creating vocabulary from token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbcc877b-9a16-4de0-98e2-c8f58f69f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab={token:integer for integer,token in enumerate(all_words)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb2dcc38-c807-420a-a8c5-94bcdbf1111e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "('\"Ah', 2)\n",
      "('\"Be', 3)\n",
      "('\"Begin', 4)\n",
      "('\"By', 5)\n",
      "('\"Come', 6)\n",
      "('\"Destroyed', 7)\n",
      "('\"Don', 8)\n",
      "('\"Gisburns\"', 9)\n",
      "('\"Grindles', 10)\n",
      "('\"Hang', 11)\n",
      "('\"Has', 12)\n",
      "('\"How', 13)\n",
      "('\"I', 14)\n",
      "('\"If', 15)\n",
      "('\"It', 16)\n",
      "('\"Jack', 17)\n",
      "('\"Money', 18)\n",
      "('\"Moon-dancers\"', 19)\n",
      "('\"Mr', 20)\n",
      "('\"Mrs', 21)\n",
      "('\"My', 22)\n",
      "('\"Never', 23)\n",
      "('\"Of', 24)\n",
      "('\"Oh', 25)\n",
      "('\"Once', 26)\n",
      "('\"Only', 27)\n",
      "('\"Or', 28)\n",
      "('\"That', 29)\n",
      "('\"The', 30)\n",
      "('\"Then', 31)\n",
      "('\"There', 32)\n",
      "('\"This', 33)\n",
      "('\"We', 34)\n",
      "('\"Well', 35)\n",
      "('\"What', 36)\n",
      "('\"When', 37)\n",
      "('\"Why', 38)\n",
      "('\"Yes', 39)\n",
      "('\"You', 40)\n",
      "('\"but', 41)\n",
      "('\"deadening', 42)\n",
      "('\"dragged', 43)\n",
      "('\"effects\"', 44)\n",
      "('\"interesting\"', 45)\n",
      "('\"lift', 46)\n",
      "('\"obituary\"', 47)\n",
      "('\"strongest', 48)\n",
      "('\"strongly\"', 49)\n",
      "('\"sweetly\"', 50)\n"
     ]
    }
   ],
   "source": [
    "for i,item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i>=50:\n",
    "     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d322dfc9-3ea8-45ab-aa82-edb2f001a2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join(self.int_to_str[i] for i in ids)\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b59a0b4-f769-4517-bf08-9ee72e64f79e",
   "metadata": {},
   "source": [
    "# üí° What is this class?\n",
    "SimpleTokenizer is a custom class made to:\n",
    "\n",
    "\n",
    "Convert text (like \"Hello, world!\") into numbers using a given dictionary (called vocab).\n",
    "\n",
    "Convert those numbers back into text.\n",
    "\n",
    "# üß± Class Structure Breakdown\n",
    "\n",
    "1. __init__(self, vocab)\n",
    "This is the constructor ‚Äî it runs when you create a new object from the class.\n",
    "vocab is a dictionary that maps words or characters to numbers.\n",
    "\n",
    "Example\n",
    "\n",
    "vocab = {\"Hello\": 1, \",\": 2, \"world\": 3, \"!\": 4}\n",
    "self.str_to_int = vocab\n",
    "‚Üí Stores the dictionary for encoding.\n",
    "\n",
    "self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "‚Üí Creates a reverse dictionary so we can decode (convert numbers back to text).\n",
    "\n",
    "# 2. encode(self, text)\n",
    "Takes a string as input (like \"Hello, world!\").\n",
    "\n",
    "Splits the text into small parts: words, spaces, and punctuation using a regex.\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "This separates:\n",
    "\n",
    "Words ‚Üí \"Hello\"\n",
    "\n",
    "Punctuation ‚Üí \",\"\n",
    "\n",
    "Spaces ‚Üí \" \" (also removed later)\n",
    "\n",
    "Removes empty or space-only parts using:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "[item.strip() for item in preprocessed if item.strip()]\n",
    "Then converts each word or symbol to its number using the vocabulary.\n",
    "\n",
    "Example: \n",
    "\"Hello, world!\" ‚Üí [\"Hello\", \",\", \"world\", \"!\"] ‚Üí [1, 2, 3, 4]\n",
    "\n",
    "\n",
    "## 3. decode(self, ids)\n",
    "Takes a list of numbers as input (like [1, 2, 3, 4]).\n",
    "\n",
    "Uses self.int_to_str to convert numbers back to words or punctuation.\n",
    "\n",
    "Joins them into a string using spaces:\n",
    "\n",
    "\"Hello , world !\"\n",
    "Cleans up spaces before punctuation using:\n",
    "\n",
    "re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "Final result: \"Hello, world!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95f602b9-35ef-4a63-af1e-dfcf5e27576e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 95, 51, 880, 1015, 633, 564, 776, 54, 1154, 627, 54, 1, 104, 56, 82, 881, 1136, 784, 823]\n"
     ]
    }
   ],
   "source": [
    "tokenizer=SimpleTokenizer(vocab)\n",
    "\n",
    "text=\"\"\"\"It's the last he painted,you know,\"\n",
    "      Mrs.Gisburn said with pardonable pride\"\"\"\n",
    "ids=tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5f94d3-d521-451f-b0c4-9be0a565e578",
   "metadata": {},
   "source": [
    "üîç item.strip() ‚Äî What is it?\n",
    "strip() is a Python string method that removes leading and trailing whitespace from a string.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bcdd77d-d3d1-40f4-9b16-73043383974d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f8141e-caaf-4f82-b78e-cb4475e85766",
   "metadata": {},
   "source": [
    "‚úÖ Final Answer to Your Question\n",
    "Even if the vocab is sorted before assigning IDs, it doesn‚Äôt matter as long as:\n",
    "\n",
    "The tokenizer remembers which token got which ID during encoding.\n",
    "\n",
    "It uses the same mapping during decoding.\n",
    "\n",
    "So, sorting only affects how IDs are assigned, not the actual sentence meaning, because:\n",
    "\n",
    "üß† The tokenizer doesn't rely on word order in the vocab; it relies on the ID-token mapping it created and stored.\n",
    "\n",
    "üìå Analogy:\n",
    "Imagine you assign numbers to words alphabetically:\n",
    "\n",
    "'Apple' = 0, 'Banana' = 1, 'Cat' = 2\n",
    "\n",
    "Your sentence: \"Banana Cat Apple\" ‚Üí [1, 2, 0]\n",
    "\n",
    "While decoding: [1, 2, 0] ‚Üí \"Banana Cat Apple\"\n",
    "\n",
    "‚úÖ It works because you use the same mapping in reverse.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4b44b0a-4918-4a90-a32e-1c7099767d15",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, do you like tea?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)   \u001b[38;5;66;03m#token is not present in the list\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m, in \u001b[0;36mSimpleTokenizer.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m      9\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m     10\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m---> 11\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_to_int[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.:;?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m     10\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip()]\n\u001b[1;32m---> 11\u001b[0m ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text=\"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))   #token is not present in the list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fbf69d-6654-4ca0-bba0-5483434395bc",
   "metadata": {},
   "source": [
    "# Adding special version token to deal new token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b00be00-1200-4223-baa4-6ec69b39f472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will add two new token unknown and the end of text\n",
    "all_token=sorted(list(set(preprocessed)))\n",
    "all_token.extend([\"<|end of text|>\",\"unk\"])\n",
    "\n",
    "vocab={token:integer for integer,token in enumerate (all_token)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0847c730-b0fa-497b-a46b-039fe334745a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1158"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ab7ec81-6250-4da6-86df-7c870aae4842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('yet', 1153)\n",
      "('you', 1154)\n",
      "('younger', 1155)\n",
      "('your', 1156)\n",
      "('yourself', 1157)\n"
     ]
    }
   ],
   "source": [
    "for i,item in enumerate (list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed7eaf6-0383-4945-a709-c74420727b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f302a77-fae7-4904-9609-00bcd56fd699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db48135a-862a-4bed-9271-082a6c05f155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "592ec9a7-f06a-4fec-acc1-5d41bec06374",
   "metadata": {},
   "source": [
    "# BYTE PAIR ENCODING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e14a2a-eeaf-46be-a623-ef0e9b7dad73",
   "metadata": {},
   "source": [
    "Byte Pair Encoding (BPE) is a simple and effective data compression and tokenization technique, especially popular in Natural Language Processing (NLP). It works by iteratively replacing the most frequent pair of bytes (or characters) in a sequence with a single, unused byte or symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a7e965f-9f01-495d-82cf-951f7d8e426d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kushagra\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf67f995-b067-43e1-ada4-35ac0df2c8f6",
   "metadata": {},
   "source": [
    "### üß† What is tiktoken?\n",
    "\n",
    "tiktoken is a fast tokenizer library developed by OpenAI for encoding text into tokens ‚Äî specifically designed to work with OpenAI models like GPT-3, GPT-3.5, and GPT-4.\n",
    "\n",
    "\n",
    "### üöÄ Key Purpose:\n",
    "\n",
    "To convert text into tokens (smallest units like words or subwords) and back from tokens to text, which is essential for LLMs like GPT to process input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f3d1dab-1026-4759-b8b9-1adccdff7cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiktoken version  0.9.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"Tiktoken version \", importlib.metadata.version(\"tiktoken\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "635f0a11-ba19-4140-8985-a7d423b62bce",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tiktoken' has no attribute 'get_encoded'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokenizer\u001b[38;5;241m=\u001b[39m \u001b[43mtiktoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_encoded\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tiktoken' has no attribute 'get_encoded'"
     ]
    }
   ],
   "source": [
    "tokenizer= tiktoken.get_encoded(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fd858a-786c-4cb6-b29a-a7262b50cf87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
